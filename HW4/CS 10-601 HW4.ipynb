{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 10-601 HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "According to $\\epsilon \\sim {\\cal N}\\left( {0,{\\sigma ^2}} \\right)$ and $y = {\\omega _0} + {\\omega _1}{x_1} + {\\omega _2}{x_2} + {\\omega _3}x_1^2 + \\epsilon$, we get \n",
    "\n",
    "$$P\\left(y \\mid x_{1}, x_{2}\\right) \\sim {\\cal N}\\left( {{\\omega _0} + {\\omega _1}{x_1} + {\\omega _2}{x_2} + {\\omega _3}x_1^2,{\\sigma ^2}} \\right)$$\n",
    "\n",
    "In a word,\n",
    "\n",
    "$$ P\\left( {y\\mid {x_1},{x_2}} \\right) = \\frac{1}{{\\sqrt {2\\pi {\\sigma ^2}} }}\\exp \\left[ { - \\frac{{{{\\left( {y - {\\omega _0} - {\\omega _1}{x_1} - {\\omega _2}{x_2} - {\\omega _3}x_1^2} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Assuming that each sample is i.i.d, we will have \n",
    "\n",
    "$$\\begin{array}{l}\n",
    "P\\left( {y\\mid {x_1},{x_2}} \\right) &= \\mathop \\prod \\limits_i P\\left( {y^{\\left( i \\right)} \\mid x_1^{\\left( i \\right)},x_2^{\\left( i \\right)}} \\right)\\\\\n",
    " &= \\mathop \\prod \\limits_i \\frac{1}{{\\sqrt {2\\pi {\\sigma ^2}} }}\\exp \\left[ { - \\frac{{{{\\left( {y^{\\left( i \\right)} - {\\omega _0} - {\\omega _1}x_1^{\\left( i \\right)} - {\\omega _2}x_2^{\\left( i \\right)} - {\\omega _3}x_1^{\\left( i \\right)2}} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right]\n",
    "\\end{array}$$\n",
    "\n",
    "Then, its conditional log likelihood becomes\n",
    "$$\\begin{array}{l}\n",
    "\\log P\\left( {y\\mid {x_1},{x_2}} \\right) &= \\sum\\limits_i {\\left[ { - \\log \\frac{1}{{\\sqrt {2\\pi {\\sigma ^2}} }} - \\frac{{{{\\left( {y^{\\left( i \\right)} - {\\omega _0} - {\\omega _1}x_1^{\\left( i \\right)} - {\\omega _2}x_2^{\\left( i \\right)} - {\\omega _3}x_1^{\\left( i \\right)2}} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right]} \\\\\n",
    " &\\simeq \\frac{1}{2}\\sum\\limits_i {{{\\left( {y^{\\left( i \\right)} - {\\omega _0} - {\\omega _1}x_1^{\\left( i \\right)} - {\\omega _2}x_2^{\\left( i \\right)} - {\\omega _3}x_1^{\\left( i \\right)2}} \\right)}^2}} \n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Accordingly, the function to be mimimized will be\n",
    "\n",
    "$$ f\\left(\\omega _0, \\omega _1, \\omega _2, \\omega _3 \\right) = \\frac{1}{2}\\sum\\limits_i {{{\\left( {y - {\\omega _0} - {\\omega _1}x_1^{\\left( i \\right)} - {\\omega _2}x_2^{\\left( i \\right)} - {\\omega _3}x_1^{\\left( i \\right)2}} \\right)}^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "The gradient of $f\\left( {\\bf{\\omega }} \\right)$ is calculated as follwing:\n",
    "\n",
    "$$\\begin{array}{l}\n",
    "\\frac{{\\partial f}}{{\\partial {\\omega _0}}} =  - \\sum\\limits_i {\\left( {y^{\\left( i \\right)} - {\\omega _0} - {\\omega _1}x_1^{\\left( i \\right)} - {\\omega _2}x_2^{\\left( i \\right)} - {\\omega _3}x_1^{\\left( i \\right)2}} \\right)} \\\\\n",
    "\\frac{{\\partial f}}{{\\partial {\\omega _1}}} =  - \\sum\\limits_i {\\left( {y^{\\left( i \\right)} - {\\omega _0} - {\\omega _1}x_1^{\\left( i \\right)} - {\\omega _2}x_2^{\\left( i \\right)} - {\\omega _3}x_1^{\\left( i \\right)2}} \\right)} x_1^{\\left( i \\right)}\\\\\n",
    "\\frac{{\\partial f}}{{\\partial {\\omega _2}}} =  - \\sum\\limits_i {\\left( {y^{\\left( i \\right)} - {\\omega _0} - {\\omega _1}x_1^{\\left( i \\right)} - {\\omega _2}x_2^{\\left( i \\right)} - {\\omega _3}x_1^{\\left( i \\right)2}} \\right)} x_2^{\\left( i \\right)}\\\\\n",
    "\\frac{{\\partial f}}{{\\partial {\\omega _3}}} =  - 2\\sum\\limits_i {\\left( {y^{\\left( i \\right)} - {\\omega _0} - {\\omega _1}x_1^{\\left( i \\right)} - {\\omega _2}x_2^{\\left( i \\right)} - {\\omega _3}x_1^{\\left( i \\right)2}} \\right)} x_1^{\\left( i \\right)}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "The graident descent update rule will be\n",
    "\n",
    "$${\\bf{\\omega }} = {\\bf{\\omega }} - \\alpha {{\\bf{\\bf{\\nabla} }}_{\\bf{\\omega }}}f\\left( {\\bf{\\omega }} \\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "In logistic regression, we have $P\\left( {y = 1 \\mid {\\bf{x}}} \\right) = \\frac{1}{{1 + \\exp \\left( {{{\\bf{\\omega }}^T}{\\bf{x}}} \\right)}}$ and $P\\left( {y = 0 \\mid {\\bf{x}}} \\right) = \\frac{{\\exp \\left( {{{\\bf{\\omega }}^T}{\\bf{x}}} \\right)}}{{1 + \\exp \\left( {{{\\bf{\\omega }}^T}{\\bf{x}}} \\right)}}$. \n",
    "\n",
    "Thereby, the probability of each sample can be expressed as \n",
    "\n",
    "$$P\\left( {{y^{\\left( i \\right)}}\\mid{{\\bf{x}}^{\\left( i \\right)}}} \\right) = {\\left[ {\\frac{{\\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}{{1 + \\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}} \\right]^{{y^{\\left( i \\right)}}}}{\\left[ {\\frac{1}{{1 + \\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}} \\right]^{1 - {y^{\\left( i \\right)}}}}$$\n",
    "\n",
    "Assuming that each sample is i.i.d, we can again have\n",
    "\n",
    "$$P\\left( {y \\mid {\\bf{x}} } \\right) = \\mathop \\prod \\limits_i {\\left[ {\\frac{{\\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}{{1 + \\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}} \\right]^{{y^{\\left( i \\right)}}}}{\\left[ {\\frac{1}{{1 + \\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}} \\right]^{1 - {y^{\\left( i \\right)}}}}$$\n",
    "\n",
    "and the conditional log likelihood becomes\n",
    "\n",
    "$$\\log P\\left( {y\\left| {\\bf{x}} \\right.} \\right) = \\sum\\limits_i {{y^{\\left( i \\right)}}\\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right) - \\log \\left[ {1 + \\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)} \\right]}   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Its derivatives are\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial {\\omega _0}}} = \\sum\\limits_i {\\left[ {{y^{\\left( i \\right)}} - \\frac{{\\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}{{1 + \\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}} \\right]}  = \\sum\\limits_i {\\left[ {{y^{\\left( i \\right)}} - P\\left( {{y^{\\left( i \\right)}} = 0\\left| {{{\\bf{x}}^{\\left( i \\right)}}} \\right.} \\right)} \\right]} $$\n",
    "\n",
    "$$\\frac{{\\partial f}}{{\\partial {\\omega _j}}} = \\sum\\limits_i {\\left[ {{y^{\\left( i \\right)}} - \\frac{{\\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}{{1 + \\exp \\left( {{{\\bf{\\omega }}^T}{{\\bf{x}}^{\\left( i \\right)}}} \\right)}}} \\right]} {x_j} = \\sum\\limits_i {\\left[ {{y^{\\left( i \\right)}} - P\\left( {{y^{\\left( i \\right)}} = 0\\left| {{{\\bf{x}}^{\\left( i \\right)}}} \\right.} \\right)} \\right]} x_j^{\\left( i \\right)}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
